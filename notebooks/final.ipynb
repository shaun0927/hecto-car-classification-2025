{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91826295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì…€ 1 : ê¸°ë³¸ ê²½ë¡œ ì„¤ì • & ì‹œë“œ ê³ ì •\n",
    "import os, random, numpy as np, torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"The number of unique classes is greater than 50% of the number of samples\",\n",
    "    category=UserWarning,\n",
    "    module=r\"sklearn\\.\")     # sklearn ê³„ì—´ ëª¨ë“ˆì—ë§Œ ì ìš©\n",
    "\n",
    "# ì ˆëŒ€ ê²½ë¡œ (Windows)\n",
    "ROOT       = \"/car2\"\n",
    "TRAIN_DIR  = os.path.join(ROOT, \"data\", \"train\")\n",
    "TEST_DIR   = os.path.join(ROOT, \"data\", \"test\")\n",
    "\n",
    "CFG = dict(\n",
    "    # IMG_SIZES = {0:256, 6:384, 12:512, 20:640, 28:768},\n",
    "    IMG_SIZES = {0: 256, 6: 384, 11: 512, 17: 640},\n",
    "    # BATCH_SIZES = {448: 48}, # í•´ìƒë„ë³„ ë°°ì¹˜ ì‚¬ì´ì¦ˆ\n",
    "    BATCH_SIZES = {256: 224, 384: 96, 512: 48, 640:32, 768:32},\n",
    "    # â˜…â˜…â˜… í•´ìƒë„ë³„ í•™ìŠµë¥  â˜…â˜…â˜…\n",
    "    LRS = {256: 4e-5, 384: 8e-5, 512: 6e-5, 640: 2e-5, 768: 8e-6},\n",
    "    EPOCH    = 30,\n",
    "    FT_EPOCHS = 6,\n",
    "    FINAL_IMG_SIZE = 768,\n",
    "    LR       = 4e-5,\n",
    "    FOLDS    = 5,\n",
    "    SEED     = 2025\n",
    ")\n",
    "\n",
    "def seed_everything(seed:int=42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark     = False\n",
    "\n",
    "seed_everything(CFG[\"SEED\"])\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ğŸ”§  ROOT  : {ROOT}\")\n",
    "print(f\"ğŸ–¼ï¸   Train: {TRAIN_DIR}\")\n",
    "print(f\"ğŸ–¼ï¸   Test : {TEST_DIR}\")\n",
    "print(f\"ğŸš€  Device: {device}  |  Seed: {CFG['SEED']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f916709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ ì œì™¸í•  íŒŒì¼ ëª©ë¡ --------------------------------------------------\n",
    "EXCLUDE_FILES = {\n",
    "    # ì™„ì „ ë…¸ì´ì¦ˆ\n",
    "    \"5ì‹œë¦¬ì¦ˆ_G60_2024_2025_0010.jpg\",\n",
    "    \"6ì‹œë¦¬ì¦ˆ_GT_G32_2018_2020_0018.jpg\",\n",
    "    \"7ì‹œë¦¬ì¦ˆ_G11_2016_2018_0040.jpg\",\n",
    "    \"911_992_2020_2024_0030.jpg\",\n",
    "    \"E_í´ë˜ìŠ¤_W212_2010_2016_0022.jpg\",\n",
    "    \"K5_2ì„¸ëŒ€_2016_2018_0007.jpg\",\n",
    "    \"F150_2004_2021_0018.jpg\",\n",
    "    \"G_í´ë˜ìŠ¤_W463b_2019_2025_0030.jpg\",\n",
    "    \"GLE_í´ë˜ìŠ¤_W167_2019_2024_0068.jpg\",\n",
    "    \"Q5_FY_2021_2024_0032.jpg\",\n",
    "    \"Q30_2017_2019_0075.jpg\",\n",
    "    \"Q50_2014_2017_0031.jpg\",\n",
    "    \"SM7_ë‰´ì•„íŠ¸_2008_2011_0053.jpg\",\n",
    "    \"X3_G01_2022_2024_0029.jpg\",\n",
    "    \"XF_X260_2016_2020_0023.jpg\",\n",
    "    \"ë‰´_ES300h_2013_2015_0000.jpg\",\n",
    "    \"ë‰´_G80_2025_2026_0042.jpg\", \"ë‰´_G80_2025_2026_0043.jpg\",\n",
    "    \"ë‰´_SM5_ì„í”„ë ˆì…˜_2008_2010_0033.jpg\",\n",
    "    \"ë”_ê¸°ì•„_ë ˆì´_EV_2024_2025_0078.jpg\",\n",
    "    \"ë”_ë‰´_K3_2ì„¸ëŒ€_2022_2024_0001.jpg\",\n",
    "    \"ë”_ë‰´_ê·¸ëœë“œ_ìŠ¤íƒ€ë ‰ìŠ¤_2018_2021_0078.jpg\",\n",
    "    \"ë”_ë‰´_ê·¸ëœë“œ_ìŠ¤íƒ€ë ‰ìŠ¤_2018_2021_0079.jpg\",\n",
    "    \"ë”_ë‰´_ê·¸ëœë“œ_ìŠ¤íƒ€ë ‰ìŠ¤_2018_2021_0080.jpg\",\n",
    "    \"ë”_ë‰´_ì•„ë°˜ë–¼_2014_2016_0031.jpg\",\n",
    "    \"ë”_ë‰´_íŒŒì‚¬íŠ¸_2012_2019_0067.jpg\",\n",
    "    \"ë ˆë‹ˆê²Œì´ë“œ_2019_2023_0041.jpg\",\n",
    "    \"ë°•ìŠ¤í„°_718_2017_2024_0011.jpg\",\n",
    "    \"ì‹¼íƒ€í˜_TM_2019_2020_0009.jpg\",\n",
    "    \"ì•„ë°˜ë–¼_MD_2011_2014_0081.jpg\",\n",
    "    \"ì•„ë°˜ë–¼_N_2022_2023_0064.jpg\", \"ì•„ë°˜ë–¼_N_2022_2023_0035.jpg\",\n",
    "    \"ìµìŠ¤í”Œë¡œëŸ¬_2016_2017_0072.jpg\",\n",
    "    \"ì½°íŠ¸ë¡œí¬ë¥´í…Œ_2017_2022_0074.jpg\",\n",
    "    \"í”„ë¦¬ìš°ìŠ¤_4ì„¸ëŒ€_2019_2022_0052.jpg\",\n",
    "    # ì°¨ëŸ‰ ë‚´ë¶€\n",
    "    \"E_í´ë˜ìŠ¤_W212_2010_2016_0069.jpg\",\n",
    "    \"ES300h_7ì„¸ëŒ€_2019_2026_0028.jpg\",\n",
    "    \"G_í´ë˜ìŠ¤_W463_2009_2017_0011.jpg\",\n",
    "    \"GLB_í´ë˜ìŠ¤_X247_2020_2023_0008.jpg\",\n",
    "    \"GLS_í´ë˜ìŠ¤_X167_2020_2024_0013.jpg\",\n",
    "    \"K3_2013_2015_0045.jpg\",\n",
    "    \"K5_3ì„¸ëŒ€_2020_2023_0081.jpg\",\n",
    "    \"Q7_4M_2020_2023_0011.jpg\",\n",
    "    \"RAV4_5ì„¸ëŒ€_2019_2024_0020.jpg\",\n",
    "    \"S_í´ë˜ìŠ¤_W223_2021_2025_0008.jpg\", \"S_í´ë˜ìŠ¤_W223_2021_2025_0071.jpg\",\n",
    "    \"X4_F26_2015_2018_0068.jpg\",\n",
    "    \"ê·¸ëœë“œ_ì²´ë¡œí‚¤_WL_2021_2023_0018.jpg\",\n",
    "    \"ë ˆì´_2012_2017_0063.jpg\",\n",
    "    \"ë ˆì¸ì§€ë¡œë²„_5ì„¸ëŒ€_2023_2024_0030.jpg\",\n",
    "    \"ë ˆì¸ì§€ë¡œë²„_ìŠ¤í¬ì¸ _2ì„¸ëŒ€_2018_2022_0014.jpg\", \"ë ˆì¸ì§€ë¡œë²„_ìŠ¤í¬ì¸ _2ì„¸ëŒ€_2018_2022_0017.jpg\",\n",
    "    \"ë§ˆì¹¸_2019_2021_0035.jpg\",\n",
    "    \"ë¨¸ìŠ¤íƒ±_2015_2023_0086.jpg\",\n",
    "    \"ì•„ë°˜ë–¼_MD_2011_2014_0009.jpg\", \"ì•„ë°˜ë–¼_MD_2011_2014_0082.jpg\",\n",
    "    \"ì»¨í‹°ë„¨íƒˆ_GT_3ì„¸ëŒ€_2018_2023_0007.jpg\",\n",
    "    \"íƒ€ì´ì¹¸_2021_2025_0065.jpg\",\n",
    "    \"íŒŒë‚˜ë©”ë¼_2010_2016_0000.jpg\", \"íŒŒë‚˜ë©”ë¼_2010_2016_0036.jpg\",\n",
    "    \"3ì‹œë¦¬ì¦ˆ_F30_2013_2018_0036.jpg\",\n",
    "    \"4ì‹œë¦¬ì¦ˆ_F32_2014_2020_0027.jpg\",\n",
    "    \"5ì‹œë¦¬ì¦ˆ_G60_2024_2025_0056.jpg\",\n",
    "    \"7ì‹œë¦¬ì¦ˆ_F01_2009_2015_0029.jpg\", \"7ì‹œë¦¬ì¦ˆ_F01_2009_2015_0044.jpg\",\n",
    "    \"911_992_2020_2024_0006.jpg\",\n",
    "    \"C_í´ë˜ìŠ¤_W204_2008_2015_0068.jpg\",\n",
    "    \"CLS_í´ë˜ìŠ¤_C257_2019_2023_0021.jpg\",\n",
    "    # ë’·íŠ¸ë í¬ ì—´ë¦¼\n",
    "    \"Q30_2017_2019_0074.jpg\", \"ê¸€ë˜ë””ì—ì´í„°_JT_2020_2023_0075.jpg\",\n",
    "    \"ë‰´_CC_2012_2016_0001.jpg\", \"ë‰´_CC_2012_2016_0002.jpg\",\n",
    "    \"ë”_ë‰´_ì½”ë‚˜_2021_2023_0081.jpg\",\n",
    "    \"2ì‹œë¦¬ì¦ˆ_ì•¡í‹°ë¸Œ_íˆ¬ì–´ëŸ¬_U06_2022_2024_0004.jpg\",\n",
    "    \"A8_D5_2018_2023_0084.jpg\",\n",
    "}\n",
    "\n",
    "# ---------- Cell-2 : StratifiedKFold (ì´ë¯¸ì§€ ë‹¨ìœ„) --------------------\n",
    "import os, hashlib, cv2, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pickle # ìºì‹±ì„ ìœ„í•´ pickle ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
    "from tqdm.auto import tqdm # ì§„í–‰ ìƒí™© í™•ì¸ì„ ìœ„í•´ tqdm import\n",
    "\n",
    "# 1) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â‘¡  ì¤‘ë³µ ê²€ì¶œ í•´ì‹œ í•¨ìˆ˜  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def sha1(path: str) -> str:\n",
    "    # 8 ~ 10 ms/ì¥. ì¶©ë¶„íˆ ë¹ ë¦…ë‹ˆë‹¤.\n",
    "    with open(path, \"rb\") as f:\n",
    "        return hashlib.sha1(f.read()).hexdigest()\n",
    "\n",
    "TRAIN_DIR = Path(TRAIN_DIR)\n",
    "\n",
    "# 2. ìºì‹œ íŒŒì¼ ê²½ë¡œ ì •ì˜\n",
    "CACHE_PATH = Path(ROOT) / \"hash_cache.pkl\"\n",
    "\n",
    "# 3. ìºì‹œ íŒŒì¼ì´ ìˆìœ¼ë©´ ë¶ˆëŸ¬ì˜¤ê³ , ì—†ìœ¼ë©´ ìƒì„±\n",
    "if CACHE_PATH.exists():\n",
    "    print(f\"âœ… Loading hash cache from: {CACHE_PATH}\")\n",
    "    with open(CACHE_PATH, \"rb\") as f:\n",
    "        path_to_hash = pickle.load(f)\n",
    "else:\n",
    "    print(f\"âš ï¸ Hash cache not found. Creating a new one... (This will take a minute)\")\n",
    "    path_to_hash = {}\n",
    "    # ì „ì²´ ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ë¯¸ë¦¬ ìˆ˜ì§‘\n",
    "    all_img_paths = list(TRAIN_DIR.glob(\"**/*.jpg\"))\n",
    "    for img_path in tqdm(all_img_paths, desc=\"Computing Hashes\"):\n",
    "        path_to_hash[str(img_path)] = sha1(img_path)\n",
    "\n",
    "    # ë‹¤ìŒ ì‹¤í–‰ì„ ìœ„í•´ ìºì‹œ íŒŒì¼ ì €ì¥\n",
    "    with open(CACHE_PATH, \"wb\") as f:\n",
    "        pickle.dump(path_to_hash, f)\n",
    "    print(f\"âœ… Hash cache created and saved to: {CACHE_PATH}\")\n",
    "\n",
    "# 2) (alias ë°˜ì˜ëœ) í´ë˜ìŠ¤ ëª©ë¡ êµ¬ì¶•\n",
    "class_names    = sorted([p.name for p in TRAIN_DIR.iterdir() if p.is_dir()])\n",
    "cls2id        = {c: i for i, c in enumerate(class_names)}\n",
    "\n",
    "records, seen_hash = [], set()\n",
    "\n",
    "# ë””ìŠ¤í¬ë¥¼ ë‹¤ì‹œ ì½ëŠ” ëŒ€ì‹ , ë¯¸ë¦¬ ê³„ì‚°ëœ í•´ì‹œ ë”•ì…”ë„ˆë¦¬ë¥¼ ì‚¬ìš©\n",
    "for img_path_str, h in tqdm(path_to_hash.items(), desc=\"Filtering Duplicates\"):\n",
    "    img_path = Path(img_path_str)\n",
    "\n",
    "    if img_path.name in EXCLUDE_FILES:\n",
    "        continue\n",
    "\n",
    "    if h in seen_hash:\n",
    "        continue\n",
    "    seen_hash.add(h)\n",
    "\n",
    "    # íŒŒì¼ ê²½ë¡œì—ì„œ í´ë˜ìŠ¤ ì´ë¦„(í´ë”ëª…)ì„ ì¶”ì¶œ\n",
    "    class_name = img_path.parent.name\n",
    "    records.append([img_path_str, cls2id[class_name]])\n",
    "\n",
    "df = pd.DataFrame(records, columns=[\"img_path\", \"label\"])\n",
    "print(f\"#images {len(df):,} | #classes {len(class_names)}\")\n",
    "\n",
    "# 3) StratifiedKFold ----------------------------------------------------\n",
    "df[\"fold\"] = -1\n",
    "skf = StratifiedKFold(n_splits=CFG[\"FOLDS\"], shuffle=True,\n",
    "                      random_state=CFG[\"SEED\"])\n",
    "for fold, (_, val_idx) in enumerate(skf.split(df, y=df[\"label\"])):\n",
    "    df.loc[val_idx, \"fold\"] = fold\n",
    "\n",
    "print(\"\\nâ—† per-class ì´ë¯¸ì§€ ìˆ˜ by fold\")\n",
    "print(df.groupby([\"label\", \"fold\"]).size().unstack(fill_value=0).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5092eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì…€ 3 (ì¬ìˆ˜ì •) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2, numpy as np\n",
    "\n",
    "# \"convnext_base\" ì „ìš©\n",
    "# MEAN, STD = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "\n",
    "# \"convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384\" ì „ìš©\n",
    "MEAN, STD = [0.48145466, 0.4578275, 0.40821073], [0.26862954, 0.26130258, 0.27577711]\n",
    "\n",
    "IMG_MAX   = max(CFG[\"IMG_SIZES\"].values())           # 512\n",
    "\n",
    "# 1) Half-crop ----------------------------------------------------------\n",
    "def half_crop(img, **kw):\n",
    "    h, w, _ = img.shape\n",
    "    side = np.random.choice([\"top\", \"bottom\", \"left\", \"right\"])\n",
    "    if   side == \"top\":    img = img[:h//2]\n",
    "    elif side == \"bottom\": img = img[h//2:]\n",
    "    elif side == \"left\":   img = img[:, :w//2]\n",
    "    else:                  img = img[:,  w//2:]\n",
    "    return img\n",
    "half_crop_aug = A.Lambda(image=half_crop)\n",
    "\n",
    "# 2) ìš°ì¸¡-ìƒë‹¨ ë§ˆìŠ¤í‚¹ ---------------------------------------------------\n",
    "def mask_rand_tr(img, frac=(0.10, 0.25), **kw):\n",
    "    h, w, _ = img.shape\n",
    "    ph, pw = int(h * np.random.uniform(*frac)), int(w * np.random.uniform(*frac))\n",
    "    img[0:ph, w-pw:w] = 0\n",
    "    return img\n",
    "mask_ur_aug = A.Lambda(image=mask_rand_tr)\n",
    "\n",
    "# 3) Letter-box --------------------------------------------------------\n",
    "def letterbox_block(sz):\n",
    "    return [\n",
    "        A.LongestMaxSize(max_size=sz, interpolation=cv2.INTER_CUBIC),\n",
    "        A.PadIfNeeded(min_height=sz, min_width=sz,\n",
    "                      border_mode=cv2.BORDER_CONSTANT, fill=0),\n",
    "    ]\n",
    "\n",
    "# 4) build_aug ---------------------------------------------------------\n",
    "def build_aug(sz: int, phase: str = \"train\"):\n",
    "    if phase == \"train\":\n",
    "        # return A.Compose([\n",
    "        #     # (a) 10 % í™•ë¥  ìŠ¤í‹°ì»¤ ë§ˆìŠ¤í‚¹\n",
    "        #     A.OneOf([mask_ur_aug, A.NoOp()], p=0.10),\n",
    "\n",
    "        #     # (b) half-crop 20 %  vs  RandomResizedCrop 80 %\n",
    "        #     A.OneOf([\n",
    "        #         A.Lambda(image=half_crop, p=1.0),                    # half-crop\n",
    "        #         A.RandomResizedCrop(size=(sz, sz),                   # â˜… tuple!\n",
    "        #                             scale=(0.8, 1.0),\n",
    "        #                             ratio=(0.75, 1.333), p=1.0),\n",
    "        #     ], p=1.0),\n",
    "        #     # A.RandomResizedCrop(size=(sz, sz),                   # â˜… tuple!\n",
    "        #     #         scale=(0.5, 1.0),\n",
    "        #     #         ratio=(0.75, 1.333), p=1.0),\n",
    "        #     # (c) Letter-boxë¡œ ì •í™•íˆ szÃ—sz\n",
    "        #     *letterbox_block(sz),\n",
    "\n",
    "        #     # (d) ì•½í•œ ë³€í˜•\n",
    "        #     A.HorizontalFlip(p=0.25),\n",
    "        #     A.Perspective(scale=(0.05, 0.1), p=0.20),\n",
    "        #     A.OneOf([\n",
    "        #         A.ColorJitter(0.4, 0.4, 0.4, 0.1, p=1.0),\n",
    "        #         A.Affine(translate_percent=(0.05, 0.05),\n",
    "        #                  scale=(0.9, 1.1), rotate=(-15, 15), p=1.0),\n",
    "        #     ], p=1.0),\n",
    "        #     A.ToGray(p=0.10),\n",
    "\n",
    "        #     # (e) CoarseDropout â€” letter-box ì´í›„\n",
    "        #     A.CoarseDropout(\n",
    "        #         num_holes_range=(1, 2),\n",
    "        #         hole_height_range=(int(sz*0.10), int(sz*0.25)),\n",
    "        #         hole_width_range =(int(sz*0.10), int(sz*0.25)),\n",
    "        #         fill=0, p=0.50\n",
    "        #     ),\n",
    "\n",
    "        #     A.Normalize(MEAN, STD),\n",
    "        #     ToTensorV2(),\n",
    "        # ])\n",
    "\n",
    "        # 1. RandomResizedCrop ë¸”ë¡ ì •ì˜\n",
    "        rrc = A.RandomResizedCrop(\n",
    "                size=(sz, sz),\n",
    "                scale=(0.5, 1.0),  # ì›ë³¸ì˜ 30%ê¹Œì§€ ì˜ë¼ë‚´ì–´ ë¶€ë¶„ë§Œ ë³´ëŠ” í›ˆë ¨ ê°•í™”\n",
    "                ratio=(0.75, 1.333),\n",
    "                p=1.0\n",
    "            )\n",
    "\n",
    "        # 2. Letterbox ë¸”ë¡ ì •ì˜ (ê¸°ì¡´ val_tfì™€ ë™ì¼)\n",
    "        letter = A.Compose([\n",
    "                *letterbox_block(sz) # LongestMaxSize + PadIfNeeded\n",
    "            ])\n",
    "\n",
    "        return A.Compose([\n",
    "            # (a) 10 % í™•ë¥  ìŠ¤í‹°ì»¤ ë§ˆìŠ¤í‚¹\n",
    "            A.OneOf([mask_ur_aug, A.NoOp()], p=0.10),\n",
    "\n",
    "            # â˜…â˜…â˜… 50:50 í™•ë¥ ë¡œ ë‘ ë¦¬ì‚¬ì´ì§• ì „ëµ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒ â˜…â˜…â˜…\n",
    "            A.OneOf([rrc, letter], p=1.0),\n",
    "\n",
    "            # 2. ê¸°í•˜í•™ì  ë³€í™˜: ë‹¤ì–‘í•œ êµ¬ë„ì™€ ê°ë„ ëŒ€ì‘\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            # Perspectiveì™€ Affineì„ ë‚®ì€ í™•ë¥ ë¡œ í•¨ê»˜ ì‚¬ìš©í•˜ì—¬ ë³µí•©ì ì¸ ì™œê³¡ ìƒì„±\n",
    "            A.Perspective(scale=(0.05, 0.1), p=0.3),\n",
    "            A.Affine(translate_percent=0.1, scale=(0.9, 1.1), rotate=(-15, 15), shear=(-10, 10), p=0.3),\n",
    "\n",
    "            # 3. ìƒ‰ìƒ/ì¡°ëª…/ë…¸ì´ì¦ˆ ë³€í™˜: ê¹Œë‹¤ë¡œìš´ ì¡°ëª… ì¡°ê±´ ëŒ€ì‘\n",
    "            A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.05, p=0.8),\n",
    "            A.ToGray(p=0.15),\n",
    "            # ì €ì¡°ë„ í™˜ê²½ì˜ ë…¸ì´ì¦ˆì™€ ë¸”ëŸ¬ë¥¼ ì‹œë®¬ë ˆì´ì…˜\n",
    "            A.OneOf([\n",
    "                A.GaussianBlur(p=1.0),\n",
    "                A.ISONoise(p=1.0),\n",
    "            ], p=0.2),\n",
    "\n",
    "            # 4. ê°€ë ¤ì§(Occlusion) ì‹œë®¬ë ˆì´ì…˜\n",
    "            # A.Erasing(\n",
    "            #     p=0.3,\n",
    "            #     scale=(0.02, 0.25), # ì´ë¯¸ì§€ì˜ 2% ~ 25% ì˜ì—­ì„ ë¬´ì‘ìœ„ ë…¸ì´ì¦ˆë¡œ ì§€ì›€\n",
    "            #     ratio=(0.3, 3.3)\n",
    "            # ),\n",
    "            # 5. ì •ê·œí™”\n",
    "            A.Normalize(MEAN, STD),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "    # ----- val / test -----\n",
    "    return A.Compose([\n",
    "        *letterbox_block(sz),\n",
    "        A.Normalize(MEAN, STD),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "# warm-up(256) ì´ˆê¸°í™”\n",
    "train_tf = build_aug(256, \"train\")\n",
    "val_tf   = build_aug(256, \"val\")\n",
    "print(\"âœ… Albumentations pipeline ready (progressive-resize compatible)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a4aa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  ì…€ 4  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import cv2, torch, numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "\n",
    "class HardPairSampler(Sampler):\n",
    "    def __init__(self, cls2idx, hard_pairs, batch_size, total_samples, prob=0.3):\n",
    "        self.cls2idx = cls2idx\n",
    "        self.hard_pairs = list(hard_pairs) if hard_pairs else []\n",
    "        self.batch_size = batch_size\n",
    "        self.total_samples = total_samples\n",
    "        self.prob = prob\n",
    "        \n",
    "        self.all_indices = list(range(total_samples))\n",
    "\n",
    "    def __iter__(self):\n",
    "        # ì „ì²´ ë°ì´í„° ê¸¸ì´ì— ë§ì¶° ë°°ì¹˜ ê°œìˆ˜ ê³„ì‚°\n",
    "        num_batches = self.total_samples // self.batch_size\n",
    "        \n",
    "        for _ in range(num_batches):\n",
    "            # 30% í™•ë¥ ë¡œ Hard-pair ë°°ì¹˜ ìƒì„±\n",
    "            if self.hard_pairs and random.random() < self.prob:\n",
    "                batch_indices = []\n",
    "                # ë°°ì¹˜ í¬ê¸°/2 ë§Œí¼ì˜ hard pairë¥¼ ìƒ˜í”Œë§\n",
    "                for _ in range(self.batch_size // 2):\n",
    "                    # hard_pairsì—ì„œ ë¬´ì‘ìœ„ë¡œ í´ë˜ìŠ¤ ìŒ ì„ íƒ\n",
    "                    anchor_cls, positive_cls = random.choice(self.hard_pairs)\n",
    "                    \n",
    "                    # ê° í´ë˜ìŠ¤ì—ì„œ ì´ë¯¸ì§€ ì¸ë±ìŠ¤ë¥¼ í•˜ë‚˜ì”© ìƒ˜í”Œë§\n",
    "                    # ìƒ˜í”Œì´ 1ê°œë¿ì¸ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ì˜ˆì™¸ ì²˜ë¦¬\n",
    "                    anchor_idx = random.choice(self.cls2idx.get(anchor_cls, [0]))\n",
    "                    positive_idx = random.choice(self.cls2idx.get(positive_cls, [0]))\n",
    "                    \n",
    "                    batch_indices.extend([anchor_idx, positive_idx])\n",
    "                yield batch_indices\n",
    "            else:\n",
    "                # ì¼ë°˜ ë¬´ì‘ìœ„ ìƒ˜í”Œë§\n",
    "                yield random.sample(self.all_indices, self.batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_samples // self.batch_size\n",
    "\n",
    "# ---------------- Dataset ----------------------------------------------\n",
    "class CarDataset(Dataset):\n",
    "    def __init__(self, df, transform=None, is_test: bool=False):\n",
    "        self.paths  = df[\"img_path\"].tolist()\n",
    "        self.labels = None if is_test else df[\"label\"].tolist()\n",
    "        self.tf = transform;  self.is_test = is_test\n",
    "\n",
    "        # í´ë˜ìŠ¤ë³„ ì¸ë±ìŠ¤ ìºì‹œ (anchor/positive ìƒ˜í”Œë§ìš©)\n",
    "        cls2idx = defaultdict(list)\n",
    "        if not is_test:\n",
    "            for idx, lbl in enumerate(self.labels):\n",
    "                cls2idx[lbl].append(idx)\n",
    "        self.cls2idx = cls2idx\n",
    "\n",
    "    def __len__(self): return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # is_test ì²˜ë¦¬ëŠ” ê·¸ëŒ€ë¡œ\n",
    "        if self.is_test:\n",
    "            img = cv2.cvtColor(cv2.imread(self.paths[idx]), cv2.COLOR_BGR2RGB)\n",
    "            if self.tf: img = self.tf(image=img)[\"image\"]\n",
    "            return img, self.paths[idx]\n",
    "        \n",
    "        # í•­ìƒ ì´ë¯¸ì§€ í•˜ë‚˜ì™€ ë¼ë²¨ í•˜ë‚˜ë§Œ ë°˜í™˜\n",
    "        img = cv2.cvtColor(cv2.imread(self.paths[idx]), cv2.COLOR_BGR2RGB)\n",
    "        if self.tf: img = self.tf(image=img)[\"image\"]\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "\n",
    "# ---------------- CutMix / MixUp helper ---------------------------------\n",
    "def rand_bbox(W, H, lam):\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w, cut_h = int(W * cut_rat), int(H * cut_rat)\n",
    "    cx, cy = np.random.randint(W), np.random.randint(H)\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    return int(bbx1), int(bby1), int(bbx2), int(bby2)\n",
    "\n",
    "def collate_cutmix(batch, alpha: float = 1.0, prob: float = 0.1):\n",
    "    imgs, labels = list(zip(*batch))\n",
    "    imgs, labels = torch.stack(imgs), torch.tensor(labels)\n",
    "    labels2 = labels\n",
    "    \n",
    "    if np.random.rand() < prob:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "        rand_idx = torch.randperm(imgs.size(0))\n",
    "        imgs2, labels2 = imgs[rand_idx], labels[rand_idx]\n",
    "\n",
    "        _, H, W = imgs.shape[1:]\n",
    "        x1, y1, x2, y2 = rand_bbox(W, H, lam)   # â† tuple ì–¸íŒ© OK\n",
    "        imgs[:, :, y1:y2, x1:x2] = imgs2[:, :, y1:y2, x1:x2]\n",
    "\n",
    "        lam = 1.0 - (x2 - x1) * (y2 - y1) / (W * H)\n",
    "    else:\n",
    "        lam, labels2 = 1.0, labels\n",
    "\n",
    "    return imgs, labels, labels2, lam\n",
    "\n",
    "def collate_mixup(batch, alpha=0.2):\n",
    "    imgs, labels = list(zip(*batch)); imgs = torch.stack(imgs); labels = torch.tensor(labels)\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    rand_idx = torch.randperm(imgs.size(0))\n",
    "    imgs = lam*imgs + (1-lam)*imgs[rand_idx]\n",
    "    labels2 = labels[rand_idx]\n",
    "    return imgs, labels, labels2, lam\n",
    "\n",
    "def collate_plain(batch):\n",
    "    imgs, labels = list(zip(*batch))\n",
    "    return torch.stack(imgs), torch.tensor(labels)\n",
    "\n",
    "# ---------------- collate_fn dispatcher ---------------------------------\n",
    "def get_collate_fn(epoch):\n",
    "    if   epoch <= 15: return lambda b: collate_cutmix(b, alpha=1.0, prob=0.3)\n",
    "    elif epoch <= 20: return lambda b: collate_mixup(b, alpha=0.2)\n",
    "    else:             return collate_plain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7e21d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  ì…€ 5  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def make_loaders(fold:int,\n",
    "                 df_full,\n",
    "                 epoch:int,\n",
    "                 train_tf,\n",
    "                 val_tf,\n",
    "                 batch_size:int = 32,\n",
    "                 num_workers:int = 10,\n",
    "                 hard_pairs: set | None = None):   # ğŸ”¸ì¶”ê°€\n",
    "    \"\"\"\n",
    "    epoch : í˜„ì¬ epoch ë²ˆí˜¸ (ì¦ê°•Â·Collate ìŠ¤ì¼€ì¤„ìš©)\n",
    "    \"\"\"\n",
    "    train_df = df_full[df_full.fold != fold].reset_index(drop=True)\n",
    "    val_df   = df_full[df_full.fold == fold].reset_index(drop=True)\n",
    "\n",
    "    train_set = CarDataset(train_df, transform=train_tf)\n",
    "    val_set   = CarDataset(val_df,   transform=val_tf)\n",
    "\n",
    "    # 1. HardPairSampler ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "    sampler = HardPairSampler(\n",
    "        cls2idx=train_set.cls2idx,\n",
    "        hard_pairs=hard_pairs,\n",
    "        batch_size=batch_size,\n",
    "        total_samples=len(train_set)\n",
    "    )\n",
    "\n",
    "    base_collate_fn = get_collate_fn(epoch)\n",
    "\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_set,\n",
    "        batch_sampler=sampler, # â˜… shuffle ëŒ€ì‹  batch_sampler ì‚¬ìš©\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        collate_fn=base_collate_fn\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        collate_fn=collate_plain\n",
    "    )\n",
    "\n",
    "    # test_paths = sorted([os.path.join(TEST_DIR,f)\n",
    "    #                      for f in os.listdir(TEST_DIR) if f.lower().endswith(\".jpg\")])\n",
    "    # test_set = CarDataset(pd.DataFrame({\"img_path\":test_paths}),\n",
    "    #                       transform=val_tf, is_test=True)\n",
    "    # test.csvë¥¼ ì§ì ‘ ì½ì–´ ìˆœì„œë¥¼ ë³´ì¥í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "    test_df = pd.read_csv(os.path.join(ROOT, \"data\", \"test.csv\"))\n",
    "    # test.csvì˜ ê²½ë¡œê°€ ìƒëŒ€ ê²½ë¡œì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜\n",
    "    test_df['img_path'] = test_df['img_path'].apply(lambda p: os.path.join(ROOT, \"data\", p))\n",
    "    test_set = CarDataset(test_df, transform=val_tf, is_test=True)\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        collate_fn=lambda b: (torch.stack([x[0] for x in b]),\n",
    "                              [x[1] for x in b])\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ded7b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  ì…€ 6 (ëª¨ë¸ ì •ì˜) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import torch, torch.nn as nn, torch.nn.functional as F, timm\n",
    "from einops import rearrange\n",
    "\n",
    "# --------------------------- GeM Pool -----------------------------------------\n",
    "class GeM(nn.Module):\n",
    "    def __init__(self, p: float = 3.0, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.p   = nn.Parameter(torch.ones(1) * p)\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        return F.avg_pool2d(x.clamp(min=self.eps).pow(self.p),\n",
    "                            kernel_size=(x.size(-2), x.size(-1))\n",
    "                           ).pow(1.0/self.p).flatten(1)\n",
    "\n",
    "# -------------------- Sub-center ArcFace Head (k=3, s=30, m=0.25) -------------\n",
    "class ArcMarginProduct_subcenter(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features:  int,\n",
    "                 out_features: int,\n",
    "                 k: int   = 3,\n",
    "                 s: float = 30.0,   # â¬† scale 30 \n",
    "                 m: float = 0.25):  # â¬† margin 0.50 â†’ 0.25\n",
    "        super().__init__()\n",
    "        self.out_features, self.k, self.s, self.m = out_features, k, s, m\n",
    "        self.weight = nn.Parameter(torch.randn(out_features * k, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, x, label: torch.Tensor | None = None):\n",
    "        x_norm = F.normalize(x, dim=1)\n",
    "        w_norm = F.normalize(self.weight, dim=1)\n",
    "\n",
    "        cosine = F.linear(x_norm, w_norm)           # (B, out*k)\n",
    "        cosine = cosine.view(-1, self.out_features, self.k)\n",
    "        cos_max, _ = torch.max(cosine, dim=2)       # (B, out)\n",
    "\n",
    "        if label is None:               # inference (margin X)\n",
    "            return self.s * cos_max\n",
    "        # ---------- margin ì¶”ê°€ (í•™ìŠµ) ----------\n",
    "        theta   = torch.acos(cos_max.clamp(-1+1e-7, 1-1e-7))\n",
    "        cos_m   = torch.cos(theta + self.m)\n",
    "        one_hot = F.one_hot(label, self.out_features).float().to(x.device)\n",
    "        logits  = self.s * (one_hot * cos_m + (1.0 - one_hot) * cos_max)\n",
    "        return logits\n",
    "\n",
    "# --------------------------- Backbone + Head -----------------------------------\n",
    "class CarNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_classes: int,\n",
    "                 k: int = 3,\n",
    "                 s: float = 30.0,\n",
    "                 m: float = 0.25,\n",
    "                 drop_path_rate: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\n",
    "            \"convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384\",\n",
    "            pretrained=True,\n",
    "            features_only=True, # â˜… ì¤‘ìš”: ì—¬ëŸ¬ ë‹¨ê³„ì˜ íŠ¹ì§• ë§µì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•˜ë„ë¡ ì„¤ì •\n",
    "            drop_path_rate=drop_path_rate          # â¬† DropPath 0.1\n",
    "        )\n",
    "\n",
    "        # 2. ë°±ë³¸ì˜ ë§ˆì§€ë§‰ ë‘ ë‹¨ê³„ì˜ ì¶œë ¥ ì±„ë„ ìˆ˜ë¥¼ ê°€ì ¸ì˜´\n",
    "        feature_info = self.backbone.feature_info.channels()\n",
    "        # ì˜ˆ: convnext_base -> [128, 256, 512, 1024] -> ë§ˆì§€ë§‰ ë‘ ê°œëŠ” 512, 1024\n",
    "        in_dim1 = feature_info[-2] # ë‘ ë²ˆì§¸ ë§ˆì§€ë§‰ íŠ¹ì§• ë§µì˜ ì±„ë„ ìˆ˜ (ë¡œì»¬ ì •ë³´)\n",
    "        in_dim2 = feature_info[-1] # ë§ˆì§€ë§‰ íŠ¹ì§• ë§µì˜ ì±„ë„ ìˆ˜ (ê¸€ë¡œë²Œ ì •ë³´)\n",
    "\n",
    "        # 3. ê° íŠ¹ì§• ë§µì— ì ìš©í•  ë³„ë„ì˜ GeM í’€ë§ ë ˆì´ì–´ 2ê°œ ìƒì„±\n",
    "        self.pool1 = GeM(p=3)\n",
    "        self.pool2 = GeM(p=3)\n",
    "\n",
    "        # â˜…â˜…â˜… ì •ê·œí™” ë ˆì´ì–´ ì¶”ê°€ â˜…â˜…â˜…\n",
    "        # ê° íŠ¹ì§• ë²¡í„°ì˜ ì°¨ì›ì— ë§ëŠ” LayerNormì„ ê°ê° ìƒì„±í•©ë‹ˆë‹¤.\n",
    "        self.norm1 = nn.LayerNorm(in_dim1)\n",
    "        self.norm2 = nn.LayerNorm(in_dim2)\n",
    "\n",
    "        # 4. ë‘ íŠ¹ì§• ë²¡í„°ë¥¼ ì—°ê²°í•  ê²ƒì´ë¯€ë¡œ, í—¤ë“œì˜ ì…ë ¥ ì°¨ì›ì€ ë‘ ì°¨ì›ì˜ í•©\n",
    "        head_in_dim = in_dim1 + in_dim2\n",
    "\n",
    "        self.head = ArcMarginProduct_subcenter(\n",
    "            in_features=head_in_dim, \n",
    "            out_features=n_classes, \n",
    "            k=k, s=s, m=m)\n",
    "\n",
    "    def forward(self, x, label: torch.Tensor | None = None, return_feat=False):\n",
    "        # 1. ë°±ë³¸ì—ì„œ íŠ¹ì§• ë§µ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ì˜´\n",
    "        features = self.backbone(x) # [map1, map2, map3, map4]\n",
    "\n",
    "        # 2. ë§ˆì§€ë§‰ ë‘ ê°œì˜ íŠ¹ì§• ë§µì„ ê°ê° í’€ë§\n",
    "        feat1 = self.pool1(features[-2]) # ë¡œì»¬ íŠ¹ì§•\n",
    "        feat2 = self.pool2(features[-1]) # ê¸€ë¡œë²Œ íŠ¹ì§•\n",
    "\n",
    "        # â˜…â˜…â˜… ê° íŠ¹ì§• ë²¡í„°ë¥¼ ì •ê·œí™” â˜…â˜…â˜…\n",
    "        norm_feat1 = self.norm1(feat1)\n",
    "        norm_feat2 = self.norm2(feat2)\n",
    "\n",
    "        # 3. ì •ê·œí™”ëœ íŠ¹ì§• ë²¡í„°ë“¤ì„ ì—°ê²°í•©ë‹ˆë‹¤.\n",
    "        feat_combined = torch.cat([norm_feat1, norm_feat2], dim=1)\n",
    "\n",
    "        # 4. ì—°ê²°ëœ íŠ¹ì§•ìœ¼ë¡œ ë¡œì§“ ê³„ì‚°\n",
    "        logits = self.head(feat_combined, label)\n",
    "        return (logits, feat_combined) if return_feat else logits\n",
    "\n",
    "# --------------------------- ì˜ˆì‹œ ì¸ìŠ¤í„´ìŠ¤ -------------------------------------\n",
    "n_classes = len(class_names)         # 396\n",
    "model = CarNet(n_classes).to(device)\n",
    "model = model.to(memory_format=torch.channels_last)\n",
    "model = torch.compile(model)         # PyTorch â‰¥ 2.0\n",
    "\n",
    "print(\"âœ… Model initialized â€“ ConvNeXt-B (DropPath 0.1) + GeM + Sub-center ArcFace \"\n",
    "      f\"(k=3, s=30, m=0.25)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a98457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  ì…€ 7 : í•™ìŠµ ì„¸íŠ¸ì—…  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1) ëª¨ë¸ì€ ì…€ 6ì—ì„œ ì´ë¯¸ ìƒì„±ë˜ì–´ ìˆìŒ (model)\n",
    "\n",
    "# 2) Loss (ì´ˆê¸°ê°’ë§Œ, epoch ë£¨í”„ì—ì„œ 0.10â†’0.05ë¡œ ê°±ì‹ )\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# 3) Optimizer â”€â”€ Backbone lr = CFG[\"LR\"], Head lr = Ã—5\n",
    "def param_groups(model, base_lr, head_lr_mul=5):\n",
    "    back, head = [], []\n",
    "    for n,p in model.named_parameters():\n",
    "        (head if \"head\" in n else back).append(p)\n",
    "    return [{\"params\":back,  \"lr\":base_lr},\n",
    "            {\"params\":head,  \"lr\":base_lr*head_lr_mul}]\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    param_groups(model, CFG[\"LR\"]),\n",
    "    lr=CFG[\"LR\"], weight_decay=1e-2\n",
    ")\n",
    "\n",
    "\n",
    "# 5) AMP & EMA\n",
    "scaler      = torch.amp.GradScaler(enabled=(device==\"cuda\"))\n",
    "ema_decay   = 0.997\n",
    "ema_weights = [p.clone().detach() for p in model.parameters()]\n",
    "\n",
    "print(\"âœ… Optimizer ready â€“ Scheduler will be created inside the Fold loop.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69212539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  ì…€ 8  (ë³¸í•™ìŠµìš© LR Finder / ëŒë¦° í›„ ì£¼ì„ ì²˜ë¦¬) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  ë°˜ë“œì‹œ ì£¼ì„ ì²˜ë¦¬!!!!!!!!!!!!!!!!!!!!!!!!! â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# from torch_lr_finder import LRFinder           # pip install torch-lr-finder\n",
    "# tmp_loader = DataLoader(                       # ì‘ì€ ì„œë¸Œì…‹(ì˜ˆ: 2~3k ìƒ˜í”Œ)\n",
    "#     CarDataset(df.sample(3000, random_state=0).reset_index(drop=True),\n",
    "#                transform=train_tf),\n",
    "#     batch_size=CFG[\"BATCH\"], shuffle=True, num_workers=4)\n",
    "\n",
    "# lr_finder = LRFinder(model, optimizer, criterion, device=device)\n",
    "# lr_finder.range_test(tmp_loader,\n",
    "#                      start_lr=1e-5, end_lr=1e-2,\n",
    "#                      num_iter=1000)\n",
    "# lr_finder.plot()    # ê·¸ë˜í”„ í™•ì¸\n",
    "# lr_finder.reset()   # ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ë³µêµ¬\n",
    "# # â–²â–²â–² LR Finder ë â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fd06f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  ì…€ 9  (W&B ë¡œê¹… í†µí•©) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import time, math, os, numpy as np, wandb, torch\n",
    "from pathlib import Path\n",
    "from tqdm.auto       import tqdm\n",
    "from sklearn.metrics import log_loss, confusion_matrix\n",
    "from timm.layers     import DropPath\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
    "\n",
    "# â•­â”€ W&B ê¸°ë³¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
    "WANDB_PROJECT = \"hecto_car_version3_0613\"\n",
    "WANDB_RUNNAME = f\"convnextB_k5_bs\"\n",
    "TOP_K         = 300\n",
    "HEAD_MULT     = 5          # back-bone LR 1 Ã—, head LR 5 Ã—\n",
    "device        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ DropPath helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def set_drop_path(model, p: float):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, DropPath):\n",
    "            m.drop_prob = p\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ EMA Â· ê¸°íƒ€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def update_ema(model, ema_w, d):\n",
    "    with torch.no_grad():\n",
    "        for p, e in zip(model.parameters(), ema_w):\n",
    "            e.mul_(d).add_(p.data, alpha=1 - d)\n",
    "\n",
    "def param_groups(model, lr, head_mult: int = HEAD_MULT):\n",
    "    back, head = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        (head if \"head\" in n else back).append(p)\n",
    "    return [{\"params\": back,  \"lr\": lr},\n",
    "            {\"params\": head,  \"lr\": lr * head_mult}]\n",
    "\n",
    "def topk_accuracy(logits, labels, topk=(1, 5)):\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        _, pred = logits.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(labels.view(1, -1).expand_as(pred))\n",
    "        return [correct[:k].reshape(-1).float().mean().item() for k in topk]\n",
    "\n",
    "def grad_global_norm(model):\n",
    "    total = 0.0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            total += p.grad.detach().float().pow(2).sum().item()\n",
    "    return total ** 0.5\n",
    "\n",
    "def build_scheduler(optimizer, warm_iters, main_iters, eta_min=0.0):\n",
    "    \"\"\"(Warm-up â†’ Cosine) or ë‹¨ì¼ Cosine ìŠ¤ì¼€ì¤„ëŸ¬ ë°˜í™˜\"\"\"\n",
    "    if warm_iters == 0:\n",
    "        return CosineAnnealingLR(optimizer, T_max=main_iters, eta_min=eta_min)\n",
    "    warm  = LinearLR(optimizer,  start_factor=0.05, end_factor=1.0,\n",
    "                     total_iters=warm_iters)\n",
    "    cos   = CosineAnnealingLR(optimizer, T_max=main_iters, eta_min=eta_min)\n",
    "    return SequentialLR(optimizer, [warm, cos], milestones=[warm_iters])\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ train / val ë£¨í”„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def train_one_epoch(model, loader, scaler, optim, scheduler, ema_w, epoch):\n",
    "    model.train()\n",
    "    run_loss = grad_acc = 0.0\n",
    "    pbar = tqdm(loader, desc=f\"Ep{epoch:02d} â–¸ train\", leave=False)\n",
    "\n",
    "    for i, batch in enumerate(pbar, 1):\n",
    "        if len(batch) == 4:\n",
    "            x, y1, y2, lam = batch\n",
    "        else:\n",
    "            x, y1 = batch;  y2, lam = y1, 1.0\n",
    "\n",
    "        x   = x.to(device, memory_format=torch.channels_last)\n",
    "        y1, y2 = y1.to(device), y2.to(device)\n",
    "\n",
    "        with torch.amp.autocast(device_type=\"cuda\", enabled=(device==\"cuda\")):\n",
    "            if lam < 1.0:\n",
    "                logits1 = model(x, label=y1)\n",
    "                logits2 = model(x, label=y2)\n",
    "                loss = lam*criterion(logits1, y1) + (1-lam)*criterion(logits2, y2)\n",
    "            else:\n",
    "                logits = model(x, label=y1)\n",
    "                loss   = criterion(logits, y1)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optim)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        grad_acc += grad_global_norm(model)\n",
    "\n",
    "        scaler.step(optim);  scaler.update();  scheduler.step()\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        update_ema(model, ema_w, ema_decay)\n",
    "\n",
    "        run_loss += loss.item()\n",
    "        pbar.set_postfix(L=f\"{run_loss/i:.4f}\")\n",
    "\n",
    "    return run_loss/i, grad_acc/i\n",
    "\n",
    "\n",
    "def validate_one_epoch(model, loader, epoch, hard_prev):\n",
    "    model.eval()\n",
    "    vloss, logL, lblL = [], [], []\n",
    "    with torch.no_grad(), torch.amp.autocast(device_type=\"cuda\", enabled=(device==\"cuda\")):\n",
    "        for x, l in tqdm(loader, desc=f\"Ep{epoch:02d} â–¸ val\", leave=False):\n",
    "            x, l = x.to(device, memory_format=torch.channels_last), l.to(device)\n",
    "            logits, _ = model(x, label=None, return_feat=True)\n",
    "            vloss.append(criterion(logits, l).item())\n",
    "            logL.append(logits.cpu()); lblL.append(l.cpu())\n",
    "\n",
    "    logitsT = torch.cat(logL);  labelsT = torch.cat(lblL)\n",
    "    probs   = logitsT.softmax(1).numpy()\n",
    "    val_ll  = log_loss(labelsT, probs, labels=list(range(n_classes)))\n",
    "    top1, top5 = topk_accuracy(logitsT, labelsT)\n",
    "    avg_maxP   = probs.max(1).mean()\n",
    "\n",
    "    # â”€â”€ Hard-pair mining\n",
    "    cm = confusion_matrix(labelsT.numpy(), probs.argmax(1), labels=range(n_classes))\n",
    "    cm[np.diag_indices_from(cm)] = 0\n",
    "    flat = np.argpartition(cm.reshape(-1), -TOP_K)[-TOP_K:]\n",
    "    r, c = np.unravel_index(flat, cm.shape)\n",
    "    hard_new = {(int(a), int(b)) for a, b in zip(r, c) if cm[a, b] > 0}\n",
    "    new_cnt, retired_cnt = len(hard_new - hard_prev), len(hard_prev - hard_new)\n",
    "    conf_err = int(cm.sum())\n",
    "\n",
    "    return (np.mean(vloss), val_ll, top1, top5, probs, labelsT.numpy(),\n",
    "            hard_new, conf_err, avg_maxP, new_cnt, retired_cnt)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Fold-level í•˜ì´í¼íŒŒë¼ë¯¸í„° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "FT_LR_SCALE = 1.5\n",
    "SCALE_MAP   = {256:1.0, 384:1.0, 512:0.9, 640:0.5, 768:0.3}   # í•´ìƒë„ë³„ ë°°ìˆ˜\n",
    "\n",
    "fold_best, all_logits, all_labels = [], [], []\n",
    "\n",
    "for fold in range(CFG[\"FOLDS\"]):\n",
    "    hard_pairs_global = set()\n",
    "    run = wandb.init(project=WANDB_PROJECT,\n",
    "                     name=f\"{WANDB_RUNNAME}_fold{fold}\",\n",
    "                     config={**CFG, \"fold\": fold}, reinit=True)\n",
    "\n",
    "    # â”€â”€ ëª¨ë¸ & Optimizer (ì²˜ìŒì€ 256 px ê¸°ì¤€)\n",
    "    model = CarNet(len(class_names), k=3, s=30.0, m=0.25)\\\n",
    "            .to(device).to(memory_format=torch.channels_last)\n",
    "\n",
    "    base_lr_init = CFG[\"LR\"] * SCALE_MAP[256]\n",
    "    optimizer = torch.optim.AdamW(param_groups(model, base_lr_init, HEAD_MULT),\n",
    "                                  lr=base_lr_init,  weight_decay=1e-2)\n",
    "\n",
    "    scaler      = torch.amp.GradScaler(enabled=(device==\"cuda\"))\n",
    "    ema_decay   = 0.997\n",
    "    ema_weights = [p.detach().clone() for p in model.parameters()]\n",
    "    best_ll, last_ll = math.inf, None\n",
    "\n",
    "    finetune_start_epoch = CFG[\"EPOCH\"] - CFG[\"FT_EPOCHS\"]\n",
    "\n",
    "    # â”€â”€ ì²« DataLoader Â· Scheduler ìƒì„± (256 px)\n",
    "    img_sz   = 256\n",
    "    batch_sz = CFG[\"BATCH_SIZES\"][img_sz]\n",
    "\n",
    "    train_tf = build_aug(img_sz, \"train\")\n",
    "    val_tf   = build_aug(img_sz, \"val\")\n",
    "    train_loader, val_loader, _ = make_loaders(\n",
    "        fold, df_full=df, epoch=0,\n",
    "        train_tf=train_tf, val_tf=val_tf,\n",
    "        batch_size=batch_sz, num_workers=10, hard_pairs=hard_pairs_global)\n",
    "\n",
    "    steps_ep  = len(train_loader)\n",
    "    warm_it   = 3 * steps_ep\n",
    "    main_it   = max(1, (CFG[\"IMG_SIZES\"][6] - 0) * steps_ep - warm_it)  # 256 â†’ 384 êµ¬ê°„\n",
    "    scheduler = build_scheduler(optimizer, warm_it, main_it, eta_min=base_lr_init*0.1)\n",
    "\n",
    "    # â”€â”€ Epoch Loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    for ep in range(CFG[\"EPOCH\"]):\n",
    "        # ìƒíƒœ ì „í™˜(í•´ìƒë„ ë³€ê²½ / Fine-tune ì§„ì…) íŒì •\n",
    "        is_resize_epoch        = ep in CFG[\"IMG_SIZES\"]\n",
    "        is_first_finetune_ep   = (ep == finetune_start_epoch)\n",
    "\n",
    "        if is_resize_epoch or is_first_finetune_ep:\n",
    "            if is_first_finetune_ep:               # Fine-tune(768 px)\n",
    "                img_sz   = CFG[\"FINAL_IMG_SIZE\"]\n",
    "                batch_sz = CFG[\"BATCH_SIZES\"][img_sz]\n",
    "                base_lr  = CFG[\"LR\"] * SCALE_MAP[768] * FT_LR_SCALE\n",
    "                print(f\"âœ¨ Epoch {ep}: Entering Fine-tune at {img_sz}px\")\n",
    "            else:                                   # Progressive-resize\n",
    "                img_sz   = CFG[\"IMG_SIZES\"][ep]\n",
    "                batch_sz = CFG[\"BATCH_SIZES\"][img_sz]\n",
    "                base_lr  = CFG[\"LR\"] * SCALE_MAP.get(img_sz, 0.3)\n",
    "                print(f\"âœ¨ Epoch {ep}: Resize â†’ {img_sz}px | Base LR {base_lr:.2e}\")\n",
    "\n",
    "            # Optimizer LR ê°±ì‹ \n",
    "            for i, pg in enumerate(optimizer.param_groups):\n",
    "                pg[\"lr\"] = base_lr if i == 0 else base_lr * HEAD_MULT\n",
    "                pg[\"initial_lr\"] = pg[\"lr\"]\n",
    "\n",
    "            # DataLoader ì¬ìƒì„±\n",
    "            train_tf = build_aug(img_sz, \"train\")\n",
    "            val_tf   = build_aug(img_sz, \"val\")\n",
    "            train_loader, val_loader, _ = make_loaders(\n",
    "                fold, df_full=df, epoch=ep,\n",
    "                train_tf=train_tf, val_tf=val_tf,\n",
    "                batch_size=batch_sz, num_workers=10,\n",
    "                hard_pairs=hard_pairs_global)\n",
    "\n",
    "            steps_ep  = len(train_loader)\n",
    "            if is_first_finetune_ep:\n",
    "                # EMA ì¬ì´ˆê¸°í™”\n",
    "                for e, p in zip(ema_weights, model.parameters()):\n",
    "                    e.copy_(p.detach())\n",
    "                warm_it = 0\n",
    "                main_it = (CFG[\"EPOCH\"] - ep) * steps_ep\n",
    "                eta_min = 0.0\n",
    "            else:\n",
    "                # í˜„ ìŠ¤í…Œì´ì§€ ì”ì—¬ ì—í­ ê³„ì‚°\n",
    "                next_switch_ep = min(\n",
    "                    [k for k in list(CFG[\"IMG_SIZES\"].keys())+[finetune_start_epoch] if k > ep] or [CFG[\"EPOCH\"]])\n",
    "                stage_epochs = next_switch_ep - ep\n",
    "                warm_it = 3 * steps_ep\n",
    "                main_it = max(1, stage_epochs*steps_ep - warm_it)\n",
    "                eta_min = base_lr * 0.1\n",
    "\n",
    "            scheduler = build_scheduler(optimizer, warm_it, main_it, eta_min)\n",
    "\n",
    "        # Loss í•¨ìˆ˜(LS ê°€ë³€)\n",
    "        if ep >= finetune_start_epoch:\n",
    "            model.head.m = 0.0\n",
    "            criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.0)\n",
    "        else:\n",
    "            criterion = torch.nn.CrossEntropyLoss(label_smoothing=(0.05 if ep <= 15 else 0.0))\n",
    "\n",
    "        # DropPath\n",
    "        set_drop_path(model.backbone, 0.1 * ep / CFG[\"EPOCH\"])\n",
    "\n",
    "        # Train / Val\n",
    "        tr_loss, tr_gn = train_one_epoch(model, train_loader, scaler,\n",
    "                                         optimizer, scheduler, ema_weights, ep)\n",
    "\n",
    "        vl_loss, vl_ll, top1, top5, vl_probs, vl_lbls,\\\n",
    "        hard_pairs_global, conf_err, avg_maxP, hp_new, hp_ret = \\\n",
    "            validate_one_epoch(model, val_loader, ep, hard_pairs_global)\n",
    "\n",
    "        logdiff = 0.0 if last_ll is None else (vl_ll - last_ll)\n",
    "        last_ll = vl_ll\n",
    "\n",
    "        wandb.log({\n",
    "            \"epoch\": ep, \"train_loss\": tr_loss, \"val_loss\": vl_loss,\n",
    "            \"logloss\": vl_ll, \"logloss_diff\": logdiff,\n",
    "            \"loss_gap\": abs(tr_loss-vl_loss),\n",
    "            \"top1\": top1, \"top5\": top5,\n",
    "            \"conf_err\": conf_err, \"avg_maxP_val\": avg_maxP,\n",
    "            \"hard_pairs\": len(hard_pairs_global),\n",
    "            \"hard_pairs/new\": hp_new, \"hard_pairs/retired\": hp_ret,\n",
    "            \"grad_norm\": tr_gn,\n",
    "            \"lr\": scheduler.get_last_lr()[0],\n",
    "        })\n",
    "\n",
    "        print(f\"[Ep{ep:02d}] Train {tr_loss:.4f} | Val {vl_loss:.4f} | \"\n",
    "              f\"LL {vl_ll:.5f} Î”{logdiff:+.5f} | Top1 {top1*100:.2f}% | \"\n",
    "              f\"HP {len(hard_pairs_global)}(+{hp_new}/-{hp_ret}) | \"\n",
    "              f\"ConfErr {conf_err} | LR {scheduler.get_last_lr()[0]:.2e}\")\n",
    "\n",
    "        if vl_ll < best_ll:\n",
    "            best_ll = vl_ll\n",
    "            torch.save({\n",
    "                \"model\": model.state_dict(),\n",
    "                \"ema\"  : [w.cpu() for w in ema_weights]},\n",
    "                f\"{ROOT}/best_model_fold{fold}.pth\")\n",
    "            wandb.save(f\"{ROOT}/best_model_fold{fold}.pth\", base_path=ROOT)\n",
    "\n",
    "    fold_best.append(best_ll)\n",
    "    all_logits.append(vl_probs);  all_labels.append(vl_lbls)\n",
    "    run.finish()\n",
    "    print(f\"ğŸ Fold {fold} best LL {best_ll:.4f}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€ OOF ì €ì¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ROOT = Path(ROOT)\n",
    "np.save(ROOT / \"oof_logits.npy\",  np.concatenate(all_logits).astype(np.float32))\n",
    "np.save(ROOT / \"oof_labels.npy\", np.concatenate(all_labels).astype(np.int32))\n",
    "print(\"ğŸ”š CV mean LogLoss :\", f\"{np.mean(fold_best):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0536d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Cell 10-prep : train â†“ embeds  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import torch, numpy as np, os\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT  = Path(ROOT)\n",
    "BATCH = 96\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "n_cls  = len(class_names)\n",
    "\n",
    "# 0) í—¬í¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def backbone_forward_feat(bk, x):\n",
    "    \"\"\"\n",
    "    timm backbone ì´\n",
    "      â€¢ ì¼ë°˜ ëª¨ë¸        â†’ forward_features(x)  ë°˜í™˜ (B,C,H,W)\n",
    "      â€¢ FeatureListNet   â†’ bk(x)[-1]           ë°˜í™˜ (B,C,H,W)\n",
    "    ë¡œ ê°€ë³€ì ì´ë¯€ë¡œ, ê³µí†µ ì¸í„°í˜ì´ìŠ¤ë¡œ ë¬¶ì–´ ì¤€ë‹¤.\n",
    "    \"\"\"\n",
    "    if hasattr(bk, \"forward_features\"):\n",
    "        feat = bk.forward_features(x)\n",
    "    else:                               # FeatureListNet\n",
    "        out = bk(x)                     # list[Tensor] or tuple\n",
    "        feat = out[-1]                  # ë§ˆì§€ë§‰ stage\n",
    "    return feat                         # (B,C,H,W)\n",
    "\n",
    "# util.py ------------------------------------------\n",
    "def extract_feat(model: CarNet, x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    í•™ìŠµ ì‹œ head ì…ë ¥ê³¼ ë™ì¼í•œ (concatÂ·norm í¬í•¨) feature ë°˜í™˜.\n",
    "    \"\"\"\n",
    "    _, feat = model(x, label=None, return_feat=True)\n",
    "    return feat          # (B, C1+C2)\n",
    "\n",
    "\n",
    "# 1) Dataset (ì¦ê°• X, val_tf ë¡œ ì¶©ë¶„)\n",
    "train_set = CarDataset(df, transform=val_tf)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=BATCH, shuffle=False,\n",
    "    num_workers=8, pin_memory=True,\n",
    "    collate_fn=lambda b: torch.stack([t[0] for t in b]))\n",
    "\n",
    "# 2) backbone (fold0 ëª¨ë¸ == feature space í†µì¼)\n",
    "ckpt  = torch.load(ROOT / \"best_model_fold0.pth\", map_location=device)\n",
    "state = {k.replace(\"_orig_mod.\",\"\"): v for k,v in ckpt[\"model\"].items()}\n",
    "\n",
    "net = CarNet(n_cls, k=3).to(device).eval().to(memory_format=torch.channels_last)\n",
    "net.load_state_dict(state, strict=True)\n",
    "\n",
    "embeds = []\n",
    "with torch.no_grad(), torch.amp.autocast(device_type=device):\n",
    "    for x in tqdm(train_loader, desc=\"â³ extract train emb\"):\n",
    "        x = x.to(device, memory_format=torch.channels_last)\n",
    "\n",
    "        # # â”€â”€ ë³€ê²½ëœ ë¶€ë¶„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        # f = backbone_forward_feat(net.backbone, x)   # (B,C,H,W)\n",
    "        # f = net.pool(f).flatten(1)                  # (B, C)\n",
    "        # # embed BN ë“±ì´ ìˆìœ¼ë©´:  f = net.bn(f)\n",
    "\n",
    "        # â˜…â˜…â˜… CarNetì˜ forwardë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ì—¬ ìœµí•©ëœ íŠ¹ì§•(feat)ì„ ê°€ì ¸ì˜´ â˜…â˜…â˜…\n",
    "        f = extract_feat(net, x)        # â† ë‹¨ì¼ í˜¸ì¶œ\n",
    "\n",
    "        embeds.append(f.cpu().numpy())\n",
    "\n",
    "embeds = np.vstack(embeds).astype(\"float32\")          # (N, dim)\n",
    "labels  = df[\"label\"].to_numpy().astype(\"int32\")\n",
    "\n",
    "np.save(ROOT / \"train_embeds.npy\", embeds)\n",
    "np.save(ROOT / \"train_labels.npy\", labels)\n",
    "print(\"âœ… saved train_embeds.npy :\", embeds.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e609ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  ì…€ 10  (k = 20, 30 ì¶”ê°€) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  â€¢ ì´ë¯¸ êµ¬ì¶•ëœ FAISS ì¸ë±ìŠ¤ / ì„ë² ë”© ì¬ì‚¬ìš©\n",
    "#  â€¢ k âˆˆ {20, 30, 50} ê°ê°ì— ëŒ€í•´\n",
    "#      knn_prob_train_k{k}.npy\n",
    "#      knn_majority_ratio_k{k}.npy\n",
    "#    ê°€ ì—†ìœ¼ë©´ ê³„ì‚°Â·ì €ì¥, ìˆìœ¼ë©´ ê±´ë„ˆëœ€\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "from pathlib import Path\n",
    "# ------------------------------------------------------------------\n",
    "ROOT = Path(ROOT)          # â† ë¬¸ìì—´ì´ë©´ Path ë¡œ, ì´ë¯¸ Path ë©´ ê·¸ëŒ€ë¡œ\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "import faiss, torch, os, numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --------------- ì „ ë‹¨ê³„ ì‚°ì¶œë¬¼ ë¶ˆëŸ¬ì˜¤ê¸° -------------------------------------------------\n",
    "train_emb = np.load(f\"{ROOT}/train_embeds.npy\").astype(\"float32\")   # (N,dim)\n",
    "train_lbl = np.load(f\"{ROOT}/train_labels.npy\")                     # (N,)\n",
    "faiss.normalize_L2(train_emb)                                       # Cosine\n",
    "\n",
    "index_path = f\"{ROOT}/faiss_ip.index\"\n",
    "index = faiss.read_index(index_path) if os.path.exists(index_path) \\\n",
    "        else faiss.IndexFlatIP(train_emb.shape[1])\n",
    "if index.ntotal == 0:                        # ì²˜ìŒ ì‹¤í–‰ ì‹œë§Œ add\n",
    "    index.add(train_emb)\n",
    "    faiss.write_index(index, index_path)\n",
    "print(f\"ğŸ”§ FAISS index ready  â€¢ vectors = {index.ntotal}\")\n",
    "\n",
    "# ì´í•˜ ë™ì¼\n",
    "n_classes = len(class_names)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ k-NN í™•ë¥  ì‚°ì¶œ (self ì œê±°) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for K in [20, 30, 50]:\n",
    "    prob_file  = ROOT / f\"knn_prob_train_k{K}.npy\"\n",
    "    ratio_file = ROOT / f\"knn_majority_ratio_k{K}.npy\"\n",
    "\n",
    "    if prob_file.exists() and ratio_file.exists():\n",
    "        print(f\"âœ… k={K} already exists â€“ skipped\")\n",
    "        continue\n",
    "\n",
    "    print(f\"â‡¢ computing k-NN  (k={K}) â€¦\")\n",
    "\n",
    "    D, I = index.search(train_emb, K + 1)   # self í¬í•¨ K+1\n",
    "    I = I[:, 1:]                            # self drop\n",
    "\n",
    "    knn_prob  = np.zeros((len(train_lbl), n_classes), dtype=np.float32)\n",
    "    for n, nbr in enumerate(I):\n",
    "        cls, cnt         = np.unique(train_lbl[nbr], return_counts=True)\n",
    "        knn_prob[n, cls] = cnt / K\n",
    "\n",
    "    maj_ratio = knn_prob.max(1)\n",
    "\n",
    "    np.save(prob_file,  knn_prob)\n",
    "    np.save(ratio_file, maj_ratio)\n",
    "    print(f\"  â€¢ saved {prob_file.name} | majority_ratio mean {maj_ratio.mean():.4f}\")\n",
    "\n",
    "print(\"\\nğŸ  k-NN probability files regenerated without self-neighbor.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0deeb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  ì…€ 12 : OOF  â€–  flip-TTA + Global-T â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import os, numpy as np, pandas as pd, torch, optuna, joblib, kornia\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.metrics import log_loss\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT       = Path(ROOT)\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "FOLDS      = CFG[\"FOLDS\"]\n",
    "IMG_SIZE = CFG[\"FINAL_IMG_SIZE\"]\n",
    "N_CLASSES  = len(class_names)\n",
    "\n",
    "val_tf = build_aug(IMG_SIZE, phase=\"val\")          # = train val transform\n",
    "\n",
    "# ---------- Dataset ----------\n",
    "class CarDatasetOOF(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, i):\n",
    "        row = self.df.iloc[i]\n",
    "        img = Image.open(row.img_path).convert(\"RGB\")\n",
    "        img = val_tf(image=np.array(img))[\"image\"]\n",
    "        # row.name ëŒ€ì‹ , 'index' ì—´ì— ì €ì¥ëœ ì›ë˜ ì¸ë±ìŠ¤ ê°’ì„ ë°˜í™˜\n",
    "        return img, row.label, row['index']            # row.name = ì›ë˜ ì¸ë±ìŠ¤\n",
    "\n",
    "def collate(batch):\n",
    "    return (torch.stack([b[0] for b in batch]),\n",
    "            torch.tensor([b[1] for b in batch]),\n",
    "            torch.tensor([b[2] for b in batch]))\n",
    "\n",
    "# ---------- OOF logits (flip-TTA, EMA X) ----------\n",
    "df_full    = df\n",
    "oof_logits = np.empty((len(df_full), N_CLASSES), np.float32)\n",
    "oof_labels = df_full.label.to_numpy().astype(np.int32)\n",
    "\n",
    "for f in range(FOLDS):\n",
    "    print(f\"ğŸ”  OOF â€“ fold {f}\")\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        CarDatasetOOF(df_full[df_full.fold == f].reset_index()), BATCH, False,\n",
    "        num_workers=10, pin_memory=True, collate_fn=collate)\n",
    "\n",
    "    ckpt  = torch.load(ROOT/f\"best_model_fold{f}.pth\", map_location=DEVICE)\n",
    "    state = {k.replace(\"_orig_mod.\",\"\"):v for k,v in ckpt[\"model\"].items()}\n",
    "    model = CarNet(N_CLASSES, k=3, s=30.0, m=0.25)\\\n",
    "            .to(DEVICE).to(memory_format=torch.channels_last).eval()\n",
    "    model.load_state_dict(state, strict=True)\n",
    "    # EMA ê°€ì¤‘ì¹˜ ë° ë²„í¼ ë®ì–´ì“°ê¸°\n",
    "    for p_ema, p in zip(ckpt[\"ema\"], model.parameters()):\n",
    "        p.data.copy_(p_ema.to(DEVICE))\n",
    "\n",
    "    if \"ema_buf\" in ckpt and len(ckpt[\"ema_buf\"]) == len(list(model.buffers())):\n",
    "        for b_ema, b in zip(ckpt[\"ema_buf\"], model.buffers()):\n",
    "            b.data.copy_(b_ema.to(DEVICE))\n",
    "\n",
    "    with torch.no_grad(), torch.amp.autocast(device_type=DEVICE, enabled=True):\n",
    "        for x, _, idx in tqdm(loader, leave=False):\n",
    "            x = x.to(DEVICE, memory_format=torch.channels_last)\n",
    "            log1 = model(x)\n",
    "            log2 = model(torch.flip(x, dims=[3]))      # h-flip\n",
    "            oof_logits[idx.numpy()] = ((log1 + log2) / 2).cpu().numpy()\n",
    "    del model; torch.cuda.empty_cache()\n",
    "\n",
    "np.save(ROOT/\"oof_logits_raw.npy\", oof_logits.astype(np.float32))\n",
    "np.save(ROOT/\"oof_labels.npy\",     oof_labels)\n",
    "\n",
    "# ---------- Global-T ë‹¨ì¼ ìµœì í™” ----------\n",
    "def obj_global(trial):\n",
    "    T = trial.suggest_float(\"T\", 0.2, 4.0, log=True)\n",
    "    prob = torch.softmax(torch.tensor(oof_logits)/T, 1).numpy()\n",
    "    return log_loss(oof_labels, prob, labels=np.arange(N_CLASSES))\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\",\n",
    "                            sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(obj_global, n_trials=200, show_progress_bar=False)\n",
    "T_global = study.best_params[\"T\"]; LL_global = study.best_value\n",
    "print(f\"ğŸŒ¡ï¸ Global T = {T_global:.4f} | OOF LL = {LL_global:.6f}\")\n",
    "\n",
    "# ---------- í™•ë¥  & T ì €ì¥ ----------\n",
    "np.save(ROOT/\"best_Ts.npy\", np.array([T_global], np.float32))   # ê¸¸ì´ 1\n",
    "prob_oof = torch.softmax(torch.tensor(oof_logits)/T_global,1)\\\n",
    "                 .numpy().astype(np.float32)\n",
    "np.save(ROOT/\"oof_logits_tta.npy\", prob_oof)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d0f72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  ì…€ 13 : Test  â€– flip-TTA + Global-T â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import torch, numpy as np, os, pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "ROOT     = Path(ROOT)\n",
    "DEVICE   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "N_FOLDS  = CFG[\"FOLDS\"];  N_CLASS = len(class_names)\n",
    "BATCH = 32\n",
    "val_tf = build_aug(IMG_SIZE, phase=\"val\")\n",
    "\n",
    "# test_paths = sorted([os.path.join(TEST_DIR,f)\n",
    "#                      for f in os.listdir(TEST_DIR) if f.lower().endswith(\".jpg\")])\n",
    "# test_set = CarDataset(pd.DataFrame({\"img_path\":test_paths}),\n",
    "#                       transform=val_tf, is_test=True)\n",
    "\n",
    "# test.csvë¥¼ ì§ì ‘ ì½ì–´ ì œì¶œ ìˆœì„œë¥¼ ì •í™•íˆ ë§ì¶°ì•¼ í•©ë‹ˆë‹¤.\n",
    "test_df = pd.read_csv(os.path.join(ROOT, \"data\", \"test.csv\"))\n",
    "# test.csvì˜ ê²½ë¡œê°€ ìƒëŒ€ ê²½ë¡œì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜\n",
    "test_df['img_path'] = test_df['img_path'].apply(lambda p: os.path.join(ROOT, \"data\", p))\n",
    "\n",
    "test_set = CarDataset(test_df, transform=val_tf, is_test=True)\n",
    "\n",
    "loader = DataLoader(test_set, BATCH, False, num_workers=10, pin_memory=True,\n",
    "                    collate_fn=lambda b: torch.stack([x[0] for x in b]))\n",
    "\n",
    "T_global = float(np.load(ROOT/\"best_Ts.npy\"))        # 0-D ë˜ëŠ” ê¸¸ì´ 1\n",
    "\n",
    "probs_fold = np.zeros((N_FOLDS, len(test_set), N_CLASS), np.float32)\n",
    "\n",
    "for f in range(N_FOLDS):\n",
    "    print(f\"ğŸ”¸ Test â€“ fold {f}\")\n",
    "    ckpt  = torch.load(ROOT/f\"best_model_fold{f}.pth\", map_location=DEVICE)\n",
    "    state = {k.replace(\"_orig_mod.\",\"\"):v for k,v in ckpt[\"model\"].items()}\n",
    "    model = CarNet(N_CLASS, k=3, s=30.0, m=0.25)\\\n",
    "            .to(DEVICE).to(memory_format=torch.channels_last).eval()\n",
    "    model.load_state_dict(state, strict=True)\n",
    "    # EMA ê°€ì¤‘ì¹˜ ë° ë²„í¼ ë®ì–´ì“°ê¸°\n",
    "    for p_ema, p in zip(ckpt[\"ema\"], model.parameters()):\n",
    "        p.data.copy_(p_ema.to(DEVICE))\n",
    "\n",
    "    if \"ema_buf\" in ckpt and len(ckpt[\"ema_buf\"]) == len(list(model.buffers())):\n",
    "        for b_ema, b in zip(ckpt[\"ema_buf\"], model.buffers()):\n",
    "            b.data.copy_(b_ema.to(DEVICE))\n",
    "\n",
    "    out = np.empty((len(test_set), N_CLASS), np.float32); ofs = 0\n",
    "    with torch.no_grad(), torch.amp.autocast(device_type=DEVICE, enabled=True):\n",
    "        for x in tqdm(loader, leave=False):\n",
    "            x = x.to(DEVICE, memory_format=torch.channels_last)\n",
    "            log1 = model(x)\n",
    "            log2 = model(torch.flip(x, [3]))\n",
    "            logits = (log1 + log2) / 2\n",
    "            prob   = torch.softmax(logits / T_global, 1)\n",
    "            bsz = len(x); out[ofs:ofs+bsz] = prob.cpu().numpy(); ofs += bsz\n",
    "    probs_fold[f] = out; del model; torch.cuda.empty_cache()\n",
    "\n",
    "prob_test = probs_fold.mean(0).astype(np.float32)\n",
    "np.save(ROOT/\"test_logits.npy\",  prob_test)\n",
    "np.save(ROOT/\"test_probs_f.npy\", probs_fold)\n",
    "print(\"âœ… test_logits.npy ì €ì¥ :\", prob_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c00eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  ì…€ 14 : Test k-NN (k = 20Â·30Â·50 ë™ì‹œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import torch, numpy as np, os, pandas as pd, faiss\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader          # â˜… ì¶”ê°€\n",
    "\n",
    "# â”€â”€ íŒŒë¼ë¯¸í„° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "K_LIST      = [20, 30, 50]                    # íƒìƒ‰í•  k\n",
    "EMB_BATCH   = 96\n",
    "INDEX_PATH  = f\"{ROOT}/faiss_ip.index\"\n",
    "LABEL_PATH  = f\"{ROOT}/train_labels.npy\"\n",
    "\n",
    "# â”€â”€ í—¬í¼: timm FeatureListNet í˜¸í™˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def backbone_forward_feat(bk, x):\n",
    "    \"\"\"\n",
    "    â€¢ ì¼ë°˜ timm ëª¨ë¸    â†’ bk.forward_features(x)\n",
    "    â€¢ FeatureListNet    â†’ bk(x)[-1]\n",
    "    ë‘˜ ë‹¤ (B,C,H,W) í…ì„œë¥¼ ë°˜í™˜.\n",
    "    \"\"\"\n",
    "    return bk.forward_features(x) if hasattr(bk, \"forward_features\") else bk(x)[-1]\n",
    "\n",
    "# â”€â”€ 1. Index & train labels ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "assert os.path.exists(INDEX_PATH), \"train ì„ë² ë”© index ê°€ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "index        = faiss.read_index(INDEX_PATH)          # IP + L2-normalized\n",
    "train_labels = np.load(LABEL_PATH)                   # (N_train,)\n",
    "n_train      = index.ntotal\n",
    "print(f\"ğŸ”§  FAISS index ready  â€¢ vectors = {n_train}\")\n",
    "\n",
    "# â”€â”€ 2. í…ŒìŠ¤íŠ¸ DataLoader  (ì…€13ê³¼ ë™ì¼) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# test_paths = sorted([os.path.join(TEST_DIR, f)\n",
    "#                      for f in os.listdir(TEST_DIR) if f.lower().endswith(\".jpg\")])\n",
    "# test_df  = pd.DataFrame({\"img_path\": test_paths})\n",
    "# test_set = CarDataset(test_df, transform=val_tf, is_test=True)\n",
    "\n",
    "test_df = pd.read_csv(os.path.join(ROOT, \"data\", \"test.csv\"))\n",
    "test_df['img_path'] = test_df['img_path'].apply(lambda p: os.path.join(ROOT, \"data\", p))\n",
    "test_set = CarDataset(test_df, transform=val_tf, is_test=True)\n",
    "\n",
    "def collate(batch):                  # (img,) ë¦¬ìŠ¤íŠ¸ â†’ Tensor\n",
    "    return torch.stack([b[0] for b in batch], 0)\n",
    "\n",
    "loader = DataLoader(test_set, batch_size=EMB_BATCH, shuffle=False,\n",
    "                    num_workers=8, pin_memory=True, collate_fn=collate)\n",
    "\n",
    "n_test   = len(test_set)\n",
    "n_class  = len(class_names)\n",
    "\n",
    "# â”€â”€ 3. Backbone ë¡œë“œ (fold0 ëª¨ë¸) & í…ŒìŠ¤íŠ¸ ì„ë² ë”© ì¶”ì¶œ (1íšŒ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ckpt  = torch.load(f\"{ROOT}/best_model_fold0.pth\", map_location=device)\n",
    "state = {k.replace(\"_orig_mod.\", \"\"): v for k, v in ckpt[\"model\"].items()}\n",
    "\n",
    "embed_net = CarNet(n_class, k=3).to(device)\n",
    "embed_net.load_state_dict(state, strict=True)\n",
    "embed_net = embed_net.to(memory_format=torch.channels_last).eval()\n",
    "\n",
    "emb_test = []\n",
    "with torch.no_grad(), torch.amp.autocast(device_type=\"cuda\" if device == \"cuda\" else \"cpu\"):\n",
    "    for x in tqdm(loader, desc=\"â‡¢ extract test emb\"):\n",
    "        x = x.to(device, memory_format=torch.channels_last)\n",
    "\n",
    "        # # â˜… ë³€ê²½: FeatureListNet í˜¸í™˜\n",
    "        # f = backbone_forward_feat(embed_net.backbone, x)   # (B,C,H,W)\n",
    "        # feat = embed_net.pool(f)                           # (B,C)\n",
    "        f = extract_feat(embed_net, x)\n",
    "        emb_test.append(f.cpu().numpy())\n",
    "\n",
    "emb_test = np.vstack(emb_test).astype(\"float32\")\n",
    "faiss.normalize_L2(emb_test)                            # Cosine ê¸°ë°˜\n",
    "\n",
    "# â”€â”€ 4. k-ë³„ ê²€ìƒ‰ & í™•ë¥  ì €ì¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for K in K_LIST:\n",
    "    out_file = f\"{ROOT}/knn_prob_test_k{K}.npy\"\n",
    "    if os.path.exists(out_file):\n",
    "        print(f\"â©  {out_file} already exists â€“ skip\")\n",
    "        continue\n",
    "\n",
    "    print(f\"â‡¢ computing k-NN  (k={K}) â€¦\")\n",
    "    D, I = index.search(emb_test, K)                    # ìµœê·¼ì ‘ K ì¸ë±ìŠ¤\n",
    "\n",
    "    knn_prob = np.zeros((n_test, n_class), dtype=np.float32)\n",
    "    for n, nbr in enumerate(I):\n",
    "        cls, cnt = np.unique(train_labels[nbr], return_counts=True)\n",
    "        knn_prob[n, cls] = cnt / K\n",
    "\n",
    "    np.save(out_file, knn_prob)\n",
    "    maj_ratio = knn_prob.max(1).mean()\n",
    "    print(f\"  â€¢ saved {os.path.basename(out_file)}  | majority_ratio mean {maj_ratio:.4f}\")\n",
    "\n",
    "print(\"ğŸ k-NN probability files ready for k =\", \", \".join(map(str, K_LIST)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeef194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Cell 15 : ìµœì¢… í™•ë¥  ì‚°ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\"\"\"\n",
    "â— ì´ ì…€ì€ ëª¨ë¸ í´ë” ì•ˆì—ì„œ ë‹¨ë…ìœ¼ë¡œ ì‹¤í–‰\n",
    "   (ROOT = í˜„ì¬ ëª¨ë¸ ë””ë ‰í„°ë¦¬)  \n",
    "â— ê²°ê³¼ë¬¼: prob_test_blend.npy  â† ë‹¤ë¥¸ ëª¨ë¸ê³¼ ì•™ìƒë¸” ë‹¨ê³„ì—ì„œ ì‚¬ìš©\n",
    "\"\"\"\n",
    "import numpy as np, pandas as pd, os\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "ROOT     = Path(os.getenv(\"ROOT\", \".\"))      # ëª¨ë¸ ì „ìš© ë””ë ‰í„°ë¦¬\n",
    "USE_KNN  = True\n",
    "\n",
    "# â”€â”€ 1. í•„ìˆ˜ íŒŒì¼ ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "prob_oof   = np.load(ROOT / \"oof_logits_tta.npy\")      # (N_train,C)\n",
    "labels_oof = np.load(ROOT / \"oof_labels.npy\").astype(int)\n",
    "prob_test  = np.load(ROOT / \"test_logits.npy\")         # (N_test ,C)\n",
    "\n",
    "# class ì´ë¦„ì„ sample_submission ë¡œë¶€í„° í™•ë³´\n",
    "sample_sub = next(ROOT.rglob(\"sample_submission.csv\"))\n",
    "class_names = pd.read_csv(sample_sub, nrows=0).columns[1:].tolist()\n",
    "C = len(class_names)\n",
    "\n",
    "# â”€â”€ 2. pure-model OOF LL í™•ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "oof_ll = log_loss(labels_oof, prob_oof, labels=np.arange(C))\n",
    "print(f\"ğŸ”  OOF LogLoss (pure model) = {oof_ll:.6f}\")\n",
    "\n",
    "# â”€â”€ 3. k-NN ë¸”ë Œë“œ (ì˜µì…˜) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "prob_out = prob_test.copy()\n",
    "if USE_KNN:\n",
    "    k_list    = [15]\n",
    "    beta_grid = np.append(np.linspace(0.60, 0.95, 15), 1.00)\n",
    "    best_k = None; best_b = 1.0; best_ll = 1e9\n",
    "\n",
    "    print(\"\\nğŸ”  Grid-search Î² for k-NN blend\")\n",
    "    for k in k_list:\n",
    "        knn_oof = np.load(ROOT / f\"knn_prob_train_k{k}.npy\")\n",
    "        for b in beta_grid:\n",
    "            mix = b*prob_oof + (1-b)*knn_oof\n",
    "            mix /= mix.sum(1, keepdims=True)\n",
    "            ll  = log_loss(labels_oof, mix, labels=np.arange(C))\n",
    "            if ll < best_ll:\n",
    "                best_ll, best_b, best_k = ll, b, k\n",
    "\n",
    "    if best_k is not None and best_b < 1.0:\n",
    "        print(f\"âœ…  best k={best_k}  Î²={best_b:.2f}  OOF LL={best_ll:.6f}\")\n",
    "        knn_test = np.load(ROOT / f\"knn_prob_test_k{best_k}.npy\")\n",
    "        prob_out = best_b*prob_test + (1-best_b)*knn_test\n",
    "        prob_out /= prob_out.sum(1, keepdims=True)\n",
    "\n",
    "# â”€â”€ 4. ìµœì¢… test í™•ë¥  ì €ì¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "np.save(ROOT / \"prob_test_blend.npy\", prob_out.astype(np.float32))\n",
    "print(\"ğŸ’¾  prob_test_blend.npy saved :\", prob_out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5369387e",
   "metadata": {},
   "source": [
    "**5fold x 2 ëª¨ë¸ ì•™ìƒë¸”**\n",
    "\n",
    "ì…€15ê¹Œì§€ ëŒë¦° í›„ ëª¨ë“  ê²°ê³¼ë¬¼ì„ ì•„ë˜ì™€ ë¹„ìŠ·í•œ ë°©ì‹ìœ¼ë¡œ í´ë”ì— ëª¨ë‘ ì˜®ê¸´ í›„ ì…€16 ì‹¤í–‰\n",
    "\n",
    "/work/\n",
    " â”œâ”€ model_A/          (ex. convnext_base)\n",
    " â”‚   â”œâ”€ best_model_fold0.pth â€¦ fold4.pth\n",
    " â”‚   â”œâ”€ oof_logits_tta.npy     â† Cell12\n",
    " â”‚   â”œâ”€ best_Ts.npy\n",
    " â”‚   â”œâ”€ test_logits.npy        â† Cell13\n",
    " â”‚   â”œâ”€ knn_prob_train_k15.npy â† Cell10/14\n",
    " â”‚   â””â”€ knn_prob_test_k15.npy  â† Cell14\n",
    " â”‚   â””â”€ prob_test_blend.npy â† Cell15\n",
    " â””â”€ model_B/          (ex. swin_large)\n",
    "     â”œâ”€ best_model_fold0.pth â€¦ fold4.pth\n",
    "     â”œâ”€ oof_logits_tta.npy\n",
    "     â”œâ”€ best_Ts.npy\n",
    "     â”œâ”€ test_logits.npy\n",
    "     â”œâ”€ knn_prob_train_k15.npy\n",
    "     â””â”€ knn_prob_test_k15.npy\n",
    "     â””â”€ prob_test_blend.npy â† Cell15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501c4d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Cell 16 : ëª¨ë¸ ê°„ ì•™ìƒë¸” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\"\"\"\n",
    "â— MODELS ë¦¬ìŠ¤íŠ¸ì— â€˜prob_test_blend.npyâ€™ ê°€ ìˆëŠ” ëª¨ë¸ í´ë” ê²½ë¡œë§Œ ì¶”ê°€\n",
    "â— OOF íŒŒì¼(oof_logits_tta.npy)ì´ ì¡´ì¬í•˜ëŠ” ëª¨ë¸ë§Œ ê°€ì¤‘ì¹˜ íƒìƒ‰ ëŒ€ìƒ\n",
    "\"\"\"\n",
    "import numpy as np, pandas as pd, os, itertools\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1) ëª¨ë¸ í´ë” ê²½ë¡œ ì„¤ì •\n",
    "MODEL_DIRS = [\n",
    "    Path(\"/work/model_A\"),   # 5-fold modelA\n",
    "    Path(\"/work/model_B\"),   # 5-fold modelB\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2) íŒŒì¼ ë¡œë“œ\n",
    "probs_test  = {}\n",
    "probs_oof   = {}\n",
    "labels_oof  = None\n",
    "for d in MODEL_DIRS:\n",
    "    probs_test[d.name] = np.load(d / \"prob_test_blend.npy\")\n",
    "    oof_path = d / \"oof_logits_tta.npy\"\n",
    "    if oof_path.exists():\n",
    "        probs_oof[d.name] = np.load(oof_path)\n",
    "        if labels_oof is None:\n",
    "            labels_oof = np.load(d / \"oof_labels.npy\")\n",
    "\n",
    "model_names   = list(probs_test.keys())\n",
    "have_oof      = list(probs_oof.keys())\n",
    "C             = probs_test[model_names[0]].shape[1]\n",
    "\n",
    "print(\"ğŸ—‚  models :\", model_names)\n",
    "print(\"ğŸ—‚  with OOF :\", have_oof)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3) ê°€ì¤‘ì¹˜ ê²°ì •\n",
    "weights = {n:0.0 for n in model_names}\n",
    "\n",
    "if len(have_oof) == 0:                     # OOF ì „í˜€ ì—†ìœ¼ë©´ ë™ë“± ê°€ì¤‘\n",
    "    for n in weights: weights[n] = 1/len(weights)\n",
    "\n",
    "elif len(have_oof) == 1:                   # OOF 1ê°œë¿ â†’ weight=1\n",
    "    weights[have_oof[0]] = 1.0\n",
    "\n",
    "elif len(have_oof) == 2:                   # OOF 2ê°œ â†’ 1ì°¨ì› Î² grid\n",
    "    best_ll, best_b = 1e9, 0.5\n",
    "    betas = np.linspace(0.0,1.0,21)\n",
    "    a,b = have_oof\n",
    "    for Î² in betas:\n",
    "        mix = Î²*probs_oof[a] + (1-Î²)*probs_oof[b]\n",
    "        mix /= mix.sum(1,keepdims=True)\n",
    "        ll = log_loss(labels_oof, mix)\n",
    "        if ll < best_ll: best_ll, best_b = ll, Î²\n",
    "    weights[a] = best_b\n",
    "    weights[b] = 1-best_b\n",
    "\n",
    "else:                                      # OOF â‰¥3  â†’ ê· ë“± or ê°„ë‹¨ Optuna\n",
    "    for n in have_oof: weights[n] = 1/len(have_oof)\n",
    "\n",
    "# ì”ì—¬ ê°€ì¤‘ì¹˜ë¥¼ OOF ì—†ëŠ” ëª¨ë¸ì— ê· ë“± ë¶„ë°°\n",
    "residual = 1 - sum(weights.values())\n",
    "no_oof   = [n for n in model_names if n not in have_oof]\n",
    "for n in no_oof:\n",
    "    weights[n] = residual / len(no_oof) if no_oof else 0.0\n",
    "\n",
    "print(\"âš–ï¸  final weights :\", weights)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4) í…ŒìŠ¤íŠ¸ í™•ë¥  í•©ì‚°\n",
    "prob_final = sum(weights[n]*probs_test[n] for n in model_names)\n",
    "prob_final /= prob_final.sum(1, keepdims=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 5) sample_submission ì‘ì„±\n",
    "ss_path = next(Path(\"/work\").rglob(\"sample_submission.csv\"))\n",
    "sub_df  = pd.read_csv(ss_path)\n",
    "sub_df.iloc[:,1:] = prob_final\n",
    "sub_df.to_csv(\"submission.csv\", index=False,\n",
    "              encoding=\"utf-8-sig\", float_format=\"%.9f\")\n",
    "\n",
    "print(\"âœ…  submission.csv saved :\", sub_df.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
